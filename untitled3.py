# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Np968d4bv0U8CiADtoE4yQWSG9a7I6Ww
"""

from __future__ import annotations

import os
import sys
import warnings
from dataclasses import dataclass
from typing import Tuple, Dict, Any, Optional

import numpy as np
import pandas as pd
import optuna
from datetime import timedelta

from statsmodels.tsa.arima.model import ARIMA
from sklearn.metrics import mean_absolute_error, mean_squared_error
from neuralprophet import NeuralProphet

warnings.filterwarnings("ignore")


def ensure_packages() -> None:
    packages = {"optuna": "optuna", "neuralprophet": "neuralprophet"}
    missing = []
    for module_name in packages:
        try:
            __import__(module_name)
        except Exception:
            missing.append(packages[module_name])
    if missing:
        try:
            import subprocess
            subprocess.check_call([sys.executable, "-m", "pip", "install", *missing])
        except Exception:
            sys.exit(1)


def generate_synthetic_series(
    start_date: str = "2018-01-01",
    periods: int = 1500,
    freq: str = "D",
    seed: int = 42,
) -> pd.DataFrame:
    np.random.seed(seed)
    dates = pd.date_range(start=start_date, periods=periods, freq=freq)
    t = np.arange(periods)

    trend = 0.0008 * t
    yearly_period = 365.25
    weekly_period = 7
    seasonal_yearly = 10 * np.sin(2 * np.pi * t / yearly_period + 0.2)
    seasonal_weekly = 3 * np.sin(2 * np.pi * t / weekly_period + 0.5)
    daily_noise = np.random.normal(scale=1.2, size=periods)

    promo = np.zeros(periods, dtype=int)
    i = 0
    while i < periods:
        if np.random.rand() < 0.04:
            length = np.random.randint(3, 10)
            promo[i: min(periods, i + length)] = 1
            i += length + np.random.randint(5, 50)
        else:
            i += 1

    weather_idx = 5 * np.sin(2 * np.pi * t / 180) + np.random.normal(scale=1.0, size=periods)
    econ_idx = np.cumsum(np.random.normal(loc=0.005, scale=0.02, size=periods))

    holiday = np.zeros(periods, dtype=int)
    fixed = {"01-01", "12-25", "10-02"}
    for idx, dt in enumerate(dates):
        if dt.strftime("%m-%d") in fixed:
            holiday[idx] = 1
        if dt.weekday() == 6 and np.random.rand() < 0.25:
            holiday[idx] = 1

    base = (
        20
        + trend * 10
        + seasonal_yearly
        + seasonal_weekly
        + 2.5 * promo
        + 0.8 * weather_idx
        + 1.2 * econ_idx
        - 5.0 * holiday
        + daily_noise
    )

    amplitude = 1.0 + 0.2 * np.sin(2 * np.pi * t / 90)
    y = base * amplitude

    return pd.DataFrame(
        {
            "ds": dates,
            "y": y,
            "promo": promo,
            "weather_idx": weather_idx,
            "econ_idx": econ_idx,
            "holiday": holiday,
        }
    )


def train_evaluate_arima(
    df: pd.DataFrame,
    train_end: str,
    order: Tuple[int, int, int] = (5, 1, 0),
) -> Dict[str, Any]:
    df = df.copy()
    df.set_index("ds", inplace=True)

    train = df.loc[:train_end]
    test = df.loc[train_end:]

    model = ARIMA(train["y"], order=order)
    fitted = model.fit()

    steps = len(test)
    forecast_res = fitted.get_forecast(steps=steps)
    pred = forecast_res.predicted_mean
    pred.index = test.index

    mae = mean_absolute_error(test["y"], pred)
    rmse = np.sqrt(mean_squared_error(test["y"], pred))

    return {"model": fitted, "predictions": pred, "test": test, "mae": mae, "rmse": rmse}


@dataclass
class HPOConfig:
    n_trials: int = 30
    timeout: Optional[int] = None
    direction: str = "minimize"
    seed: int = 42
    metric: str = "MAE"


def prepare_neuralprophet_df(df: pd.DataFrame) -> pd.DataFrame:
    return df[["ds", "y", "promo", "weather_idx", "econ_idx", "holiday"]].copy()


def neuralprophet_objective(
    trial: optuna.trial.Trial,
    df_train: pd.DataFrame,
    df_val: pd.DataFrame,
    regressors: list[str],
    hparams_fixed: Optional[dict] = None,
) -> float:

    params = {
        "n_forecasts": 1,
        "n_lags": trial.suggest_int("n_lags", 0, 30),
        "learning_rate": trial.suggest_loguniform("learning_rate", 1e-4, 1e-1),
        "epochs": trial.suggest_int("epochs", 30, 200),
        "batch_size": trial.suggest_categorical("batch_size", [32, 64, 128]),
        "trend_reg": trial.suggest_loguniform("trend_reg", 1e-8, 1e-1),
        "seasonality_reg": trial.suggest_loguniform("seasonality_reg", 1e-8, 1e-1),
        "num_hidden_layers": trial.suggest_int("num_hidden_layers", 0, 3),
        "d_hidden": trial.suggest_int("d_hidden", 8, 128),
    }

    if hparams_fixed:
        params.update(hparams_fixed)

    model = NeuralProphet(
        n_lags=params["n_lags"],
        n_forecasts=params["n_forecasts"],
        learning_rate=params["learning_rate"],
        epochs=params["epochs"],
        batch_size=params["batch_size"],
        trend_reg=params["trend_reg"],
        seasonality_reg=params["seasonality_reg"],
        num_hidden_layers=params["num_hidden_layers"],
        d_hidden=params["d_hidden"],
        yearly_seasonality=True,
        weekly_seasonality=True,
        daily_seasonality=False,
        verbose=False,
    )

    for r in regressors:
        model = model.add_regressor(r, standardize=True)

    try:
        model.fit(df_train, freq="D", validation_df=df_val, progress="none")
    except Exception:
        return 1e6

    preds = model.predict(df_val)
    if "yhat1" not in preds.columns:
        return 1e6

    y_true = df_val["y"].values
    y_pred = preds["yhat1"].values[: len(y_true)]

    return mean_absolute_error(y_true, y_pred)


def run_optuna_hpo(
    df: pd.DataFrame,
    train_end: str,
    val_end: str,
    regressors: list[str],
    hpo_cfg: HPOConfig,
) -> Dict[str, Any]:

    df_train = df.loc[df["ds"] <= pd.to_datetime(train_end)]
    df_val = df.loc[(df["ds"] > pd.to_datetime(train_end)) & (df["ds"] <= pd.to_datetime(val_end))]

    study = optuna.create_study(
        direction=hpo_cfg.direction,
        sampler=optuna.samplers.TPESampler(seed=hpo_cfg.seed),
    )

    func = lambda trial: neuralprophet_objective(
        trial,
        prepare_neuralprophet_df(df_train),
        prepare_neuralprophet_df(df_val),
        regressors,
    )

    study.optimize(func, n_trials=hpo_cfg.n_trials, timeout=hpo_cfg.timeout)

    return {"study": study, "best_params": study.best_params, "best_value": study.best_value}


def train_final_neuralprophet(
    df_train_plus_val: pd.DataFrame,
    df_test: pd.DataFrame,
    regressors: list[str],
    best_params: dict,
) -> Dict[str, Any]:

    defaults = {
        "n_forecasts": 1,
        "n_lags": 0,
        "learning_rate": 1e-3,
        "epochs": 100,
        "batch_size": 64,
        "trend_reg": 1e-6,
        "seasonality_reg": 1e-6,
        "num_hidden_layers": 1,
        "d_hidden": 32,
    }

    for k, v in defaults.items():
        best_params.setdefault(k, v)

    model = NeuralProphet(
        n_lags=best_params["n_lags"],
        n_forecasts=best_params["n_forecasts"],
        learning_rate=best_params["learning_rate"],
        epochs=best_params["epochs"],
        batch_size=best_params["batch_size"],
        trend_reg=best_params["trend_reg"],
        seasonality_reg=best_params["seasonality_reg"],
        num_hidden_layers=best_params["num_hidden_layers"],
        d_hidden=best_params["d_hidden"],
        yearly_seasonality=True,
        weekly_seasonality=True,
        daily_seasonality=False,
        verbose=False,
    )

    for r in regressors:
        model = model.add_regressor(r, standardize=True)

    model.fit(prepare_neuralprophet_df(df_train_plus_val), freq="D")

    preds = model.predict(prepare_neuralprophet_df(df_test))
    y_true = df_test["y"].values
    y_pred = preds["yhat1"].values[: len(y_true)]

    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))

    return {"model": model, "predictions": y_pred, "test": df_test, "mae": mae, "rmse": rmse}


def metrics_table(results: Dict[str, Any]) -> pd.DataFrame:
    rows = [{"model": name, "MAE": r["mae"], "RMSE": r["rmse"]} for name, r in results.items()]
    return pd.DataFrame(rows).set_index("model")


def generate_text_report(
    df: pd.DataFrame,
    arima_res: Dict[str, Any],
    np_res: Dict[str, Any],
    hpo_out: Dict[str, Any],
    train_end: str,
    val_end: str,
    test_end: str,
) -> str:

    study = hpo_out["study"]
    best_params = hpo_out["best_params"]
    best_value = hpo_out["best_value"]

    rep = []
    rep.append("# NeuralProphet HPO Forecasting Report\n")
    rep.append("## Data Split\n")
    rep.append(f"Train End: {train_end} | Val End: {val_end} | Test End: {test_end}\n")

    rep.append("\n## HPO Summary\n")
    rep.append(f"Trials: {len(study.trials)}\n")
    rep.append(f"Best Val MAE: {best_value:.4f}\n")
    rep.append("Best Params:\n")
    for k, v in best_params.items():
        rep.append(f"- {k}: {v}\n")

    rep.append("\n## Test Metrics\n")
    rep.append("| Model | MAE | RMSE |\n|---|---|---|\n")
    rep.append(f"| ARIMA | {arima_res['mae']:.4f} | {arima_res['rmse']:.4f} |\n")
    rep.append(f"| NeuralProphet | {np_res['mae']:.4f} | {np_res['rmse']:.4f} |\n")

    return "".join(rep)


def main() -> None:
    df = generate_synthetic_series(start_date="2018-01-01", periods=1500, seed=42)

    train_end = (df["ds"].iloc[0] + timedelta(days=1000)).strftime("%Y-%m-%d")
    val_end = (df["ds"].iloc[0] + timedelta(days=1200)).strftime("%Y-%m-%d")
    test_end = df["ds"].iloc[-1].strftime("%Y-%m-%d")

    arima_res = train_evaluate_arima(df[["ds", "y"]], train_end, order=(5, 1, 0))

    regressors = ["promo", "weather_idx", "econ_idx", "holiday"]
    hpo_cfg = HPOConfig(n_trials=30, seed=42)
    hpo_out = run_optuna_hpo(df, train_end, val_end, regressors, hpo_cfg)

    df_train_val = df.loc[df["ds"] <= pd.to_datetime(val_end)]
    df_test = df.loc[df["ds"] > pd.to_datetime(val_end)]

    np_res = train_final_neuralprophet(df_train_val, df_test, regressors, hpo_out["best_params"])

    metrics_df = metrics_table({"ARIMA": arima_res, "NeuralProphet": np_res})

    report_text = generate_text_report(df, arima_res, np_res, hpo_out, train_end, val_end, test_end)

    out_dir = "experiment_outputs"
    os.makedirs(out_dir, exist_ok=True)
    with open(os.path.join(out_dir, "report.md"), "w") as f:
        f.write(report_text)
    metrics_df.to_csv(os.path.join(out_dir, "metrics.csv"))

    print("Experiment Completed")


if __name__ == "__main__":
    main()